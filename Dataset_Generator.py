"""
Last update on April 2025

@author: jlittaye
"""

from sqlite3 import SQLITE_DROP_TEMP_TRIGGER
import matplotlib.pyplot as plt
import numpy as np
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from sklearn.feature_extraction import image
import pandas as pd
import time
import os
import random
from math import *

from Functions import function_NPZD, Forcing_Gen, Dataset_Gen

# Source of Nitrogen of the model
global Q0
Q0 = 4+2.5+1.5+0
# Will stack the number of file
global n_sum

global device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.set_default_device(device)
print(f"Device set to {device}")



if not os.path.isdir("Loading_files") : # Contains transcient data
    os.makedirs("Loading_files") 

########################################################################
###################                                  ####################
################  Generation Brut Data to train the NN  ##################
###################                                  ####################
########################################################################
## Data is generated by "n_file = 5" packets of "file_size = 1000"

n_file = 5
file_size = 1000
nb_ensemble = 100
folder_save = "Generated_Datasets/"

for case in [0, 1, 2, 3] :
    ti_whole = time.time()
    method_name = "NN/"
    name_file = f"Case_{case}/"
    if not os.path.isdir(folder_save+method_name+name_file) :
        os.makedirs(folder_save+method_name+name_file)

    n_sum = n_file*file_size
    seeds = []
## Generating the sub_files one by one
    for nb_file in range(n_file) :
        Theta, False_forcings, True_forcings, States, seed, Phi_stats = Dataset_Gen(file_size, case, device=device, num_save = nb_file)
        seeds.append(seed)
        torch.save(Theta, f"Loading_files/Theta_{n_file*file_size}data_{nb_file}.pt")
        torch.save(Phi_stats, f"Loading_files/Phi_stats_{n_file*file_size}data_{nb_file}.pt")
        torch.save(False_forcings, f"Loading_files/False_forcings_{n_file*file_size}data_{nb_file}.pt")
        torch.save(True_forcings, f"Loading_files/True_forcings_{n_file*file_size}data_{nb_file}.pt")
        torch.save(States, f"Loading_files/States_{n_file*file_size}data_{nb_file}.pt")
        print("File nÂ°", (nb_file+1), "/", n_file, " generated")
## Once the files are generated we concatenate them
    for nb_file in range(n_file) :
        if nb_file == 0 :
            Theta = torch.load(f"Loading_files/Theta_{n_file*file_size}data_{nb_file}.pt")
            Phi_stats = torch.load(f"Loading_files/Phi_stats_{n_file*file_size}data_{nb_file}.pt")
            False_forcings = torch.load(f"Loading_files/False_forcings_{n_file*file_size}data_{nb_file}.pt")
            True_forcings = torch.load(f"Loading_files/True_forcings_{n_file*file_size}data_{nb_file}.pt")
            States = torch.load(f"Loading_files/States_{n_file*file_size}data_{nb_file}.pt")
        else :
            Theta = torch.cat((Theta, torch.load(f"Loading_files/Theta_{n_file*file_size}data_{nb_file}.pt")), dim = 0)
            Phi_stats = torch.cat((Phi_stats, torch.load(f"Loading_files/Phi_stats_{n_file*file_size}data_{nb_file}.pt")), dim = 0)
            False_forcings = torch.cat((False_forcings, torch.load(f"Loading_files/False_forcings_{n_file*file_size}data_{nb_file}.pt")), dim = 0)
            True_forcings = torch.cat((True_forcings, torch.load(f"Loading_files/True_forcings_{n_file*file_size}data_{nb_file}.pt")), dim = 0)
            States = torch.cat((States, torch.load(f"Loading_files/States_{n_file*file_size}data_{nb_file}.pt")), dim = 0)
    print("Shape of all the states: ", States.shape)

    torch.save(torch.tensor(seeds), folder_save+method_name+name_file+"seeds.pt") # Seeds for the random variables if one wants to gets the exact same HF variations
    torch.save(Theta, folder_save+method_name+name_file+"Theta.pt")
    torch.save(Phi_stats, folder_save+method_name+name_file+"Phi_stats.pt")
    torch.save(False_forcings, folder_save+method_name+name_file+"False_forcings.pt")
    torch.save(True_forcings, folder_save+method_name+name_file+"True_forcings.pt")
    torch.save(States, folder_save+method_name+name_file+"States.pt")



#########################################################
################  Generation Dataset  ####################
#########################################################
## Data are sampled (one sample per day)
    States_sampled = States[:, :, 1:].unfold(1, 1, 2)[:, :, :, 0] # Have been simulated with time-step of 1/2 day
    False_forcings_sampled = False_forcings.unfold(1, 1, 24)[:, 365*2:365*3, :, 0] # Have been simulated with time-step of 1h
    True_forcings_sampled = True_forcings.unfold(1, 1, 24)[:, 365*2:365*3, :, 0] # Have been simulated with time-step of 1h

## Concatenate states + Forcings (with uncertainties)
    inputs_sampled = torch.cat((States_sampled, False_forcings_sampled), dim = 2)

## Generate the observation error matrix
    obs_noise_perc = 1. # Percentage of the observation variance for the noise
    mask_obs_error = torch.zeros([inputs_sampled.shape[0], inputs_sampled.shape[1], 4])
    for dim_ch in range(4) :
        mask_obs_error[:, :, dim_ch] += torch.normal(0., torch.std(inputs_sampled[:, :, dim_ch]).item()*obs_noise_perc/100, (inputs_sampled.shape[0], inputs_sampled.shape[1])).to(device)

## Adding obs error to obs
    inputs_sampled_noised = torch.clone(inputs_sampled)
    inputs_sampled_noised[:, :, :4] += mask_obs_error

## Normalizing inputs (obs + forcings)
    inputs_sampled_noised_normalized = torch.zeros(inputs_sampled_noised.shape)
    mean_input = torch.mean(inputs_sampled_noised, dim = (0, 1))
    std_input = torch.std(inputs_sampled_noised, dim = (0, 1))
    for ch in range(inputs_sampled_noised_normalized.shape[2]) :
        inputs_sampled_noised_normalized[:, :, ch] = (inputs_sampled_noised[:, :, ch]-torch.mean(inputs_sampled_noised[:, :, ch]))/torch.std(inputs_sampled_noised[:, :, ch])
    inputs_sampled_noised_normalized = inputs_sampled_noised_normalized.moveaxis(1, 2)

## Normalizing outputs (parameters)
    outputs_normalized = (Theta-torch.mean(Theta, dim = 0))/torch.std(Theta, dim = 0)
    mean_output = torch.mean(Theta, dim = 0)
    std_output = torch.std(Theta, dim = 0)

## Saving mean, std, observation error
    torch.save(mean_output, folder_save+method_name+name_file+"/mean_y.pt")
    torch.save(std_output, folder_save+method_name+name_file+"/std_y.pt")
    torch.save(mean_input, folder_save+method_name+name_file+"/mean_x.pt")
    torch.save(std_input, folder_save+method_name+name_file+"/std_x.pt")

    torch.save(mask_obs_error, folder_save+method_name+name_file+"/Obs_matrix.pt")

## 80% for training, 20% for validation
    DS_train = TensorDataset(torch.clone(inputs_sampled_noised_normalized[:int(n_file*file_size*0.8-1), :, :]), torch.clone(outputs_normalized[:int(n_file*file_size*0.8-1)]))
    DS_valid = TensorDataset(torch.clone(inputs_sampled_noised_normalized[int(n_file*file_size*0.8-1):, :, :]), torch.clone(outputs_normalized[int(n_file*file_size*0.8-1):]))
    torch.save(DS_train, folder_save+method_name+name_file+"/DS_train")
    torch.save(DS_valid, folder_save+method_name+name_file+"/DS_valid")

## Deleting variables to save memory for the next generated case
    del mean_output, mean_input, std_output, std_input, mask_obs_error, inputs_sampled_noised_normalized, outputs_normalized, DS_train, DS_valid, Theta, inputs_sampled_noised, inputs_sampled, False_forcings, False_forcings_sampled, True_forcings_sampled, True_forcings

    t_whole = time.time()-ti_whole
    print(f"Training/validation data for the case {case} have been generated in {int(t_whole//3600)}h:{int((t_whole%3600)//60)}min:{int(((t_whole%3600)%60))}sec.")



###########################################################################
##################                                    ######################
################  Generation Brut Data for the DA method  ###################
##################                                    ######################
###########################################################################

    print("Start test training dataset")
    method_name = "DA/"
    ti_whole = time.time()
    if not os.path.isdir(folder_save+method_name+name_file) :
        os.makedirs(folder_save+method_name+name_file)        

    file_size = 100

    Theta, False_forcings, Ensemble_False_forcings, True_forcings, States, seeds, Phi_stats = Dataset_Gen(file_size, case, device=device, nb_ensemble = nb_ensemble, nb_annee_min = 2, nb_annee_max = 5)

    torch.save(torch.tensor(seeds), folder_save+method_name+name_file+"seeds.pt") # Seeds for the random variables if one wants to gets the exact same HF variations
    torch.save(Theta, folder_save+method_name+name_file+"Theta.pt")
    torch.save(Phi_stats, folder_save+method_name+name_file+"Phi_stats.pt")
    torch.save(False_forcings, folder_save+method_name+name_file+"False_forcings.pt")
    torch.save(Ensemble_False_forcings, folder_save+method_name+name_file+f"Ensemble_false_forcings_{nb_ensemble}.pt")
    torch.save(True_forcings, folder_save+method_name+name_file+"True_forcings.pt")
    torch.save(States, folder_save+method_name+name_file+"States.pt")


#############################################################
################  Concatenating the data  ###################
#############################################################
    ti, tf, dt = 365*0, 365*1, 1/2
## Data are sampled (one sample per day)
    States_sampled_t = States.unfold(1, 1, int(1/dt))[:, :tf, :, 0] # Have been simulated with time-step of 1/2 day

    inputs = torch.zeros([False_forcings.shape[0], int((tf-ti)/7), 5, 1])
    outputs = torch.zeros([False_forcings.shape[0], int((tf-ti)/7), 4, (1+7)])
    for i in range(inputs.shape[0]) :
        for j in range(inputs.shape[1]) :
            inputs[i, j, :, 0] = torch.tensor([States_sampled_t[i, int((ti+j*7)), 0], States_sampled_t[i, int((ti+j*7)), 1], States_sampled_t[i, int((ti+j*7)), 2], States_sampled_t[i, int((ti+j*7)), 3], States_sampled_t[i, int((ti+j*7)), 4]])
            for k in range(outputs.shape[3]) :
                outputs[i, j, :, k] = torch.tensor([States_sampled_t[i, int((ti+j*7)+k), 1][None], States_sampled_t[i, int((ti+j*7)+k), 2][None], States_sampled_t[i, int((ti+j*7)+k), 3][None], States_sampled_t[i, int((ti+j*7)+k), 4][None]])
    dataset = DataLoader(TensorDataset(inputs, outputs), batch_size = 100, shuffle = False)
    torch.save(dataset, folder_save+method_name + name_file + "/TrainLoader.pt")

## Generate the observation error matrix
    obs_noise_perc = 1. # Percentage of the observation variance for the noise
    mask_obs_error = torch.zeros([inputs.shape[0], inputs.shape[1], 4, 1])
    for dim_ch in range(4) :
        mask_obs_error[:, :, dim_ch, 0] += torch.normal(0., torch.std(inputs[:, :, dim_ch+1]).item()*obs_noise_perc/100, (inputs.shape[0], inputs.shape[1])).to(device)
    
    torch.save(mask_obs_error, folder_save+method_name+name_file+"/Obs_matrix.pt")
    


###########################################################################
##################                                    #####################
################  Generation Dataset for the NN method  ###################
##################                                    #####################
###########################################################################
    
    mean_x = torch.load(folder_save+f"NN/Case_{case}/mean_x.pt", map_location = device)
    mean_y = torch.load(folder_save+f"NN/Case_{case}/mean_y.pt", map_location = device)
    std_x = torch.load(folder_save+f"NN/Case_{case}/std_x.pt", map_location = device)
    std_y = torch.load(folder_save+f"NN/Case_{case}/std_y.pt", map_location = device)

    Obs_noise = torch.zeros([States_sampled_t.shape[0], States_sampled_t.shape[1], 4]) ## Contains the observation error in the shape [n_data_set, time_length, n_ch]
    for i in range(52) :
        Obs_noise[:, i*7:(i+1)*7, :] = mask_obs_error[:, i, :, :7].moveaxis(1, 2)

    States_sampled = torch.clone(States).unfold(1, 1, int(1/dt))[:, :tf, 1:, 0] + Obs_noise # states already cut from year 2 so states_sampled gathers years 3, 4, 5, here we take the last year

    False_forcings_sampled = False_forcings.unfold(1, 1, 24)[:, 365*2:365*3, :, 0]
    True_forcings_sampled = True_forcings.unfold(1, 1, 24)[:, 365*2:365*3, :, 0]

    ## Creates inputs : observations + physical forcing with uncertainty
    inputs_sampled = torch.cat((States_sampled, False_forcings_sampled), dim = 2)
    inputs_sampled_normalized = torch.zeros(inputs_sampled.shape)
    for ch in range(inputs_sampled_normalized.shape[2]) :
        inputs_sampled_normalized[:, :, ch] = (inputs_sampled[:, :, ch]-mean_x[ch])/std_x[ch]
    ## Normalize the output (theta)
    outputs_normalized = (Theta-mean_y)/std_y

    DS_test_NN = TensorDataset(inputs_sampled_normalized.moveaxis(1, 2), outputs_normalized)
    torch.save(DS_test_NN, folder_save+method_name+name_file+"/DS_test_NN")


    ## Creates inputs for the ensemble
    False_forcings_sampled_ensemble = Ensemble_False_forcings.unfold(1, 1, 24)[:, 365*2:365*3, :, :, 0]
    States_sampled_ensemble = States_sampled[:, :, :, None].repeat(1, 1, 1, nb_ensemble)

    inputs_sampled_ensemble = torch.cat((States_sampled_ensemble, False_forcings_sampled_ensemble), dim = 2)
    inputs_sampled_normalized_ensemble = torch.zeros(inputs_sampled_ensemble.shape)
    for ch in range(inputs_sampled_normalized_ensemble.shape[2]) :
        inputs_sampled_normalized_ensemble[:, :, ch] = (inputs_sampled_ensemble[:, :, ch]-mean_x[ch])/std_x[ch]
    
    DS_test_NN_ensemble = TensorDataset(inputs_sampled_normalized_ensemble.moveaxis(1, 2), outputs_normalized)
    torch.save(DS_test_NN_ensemble, folder_save+f"DA/Case_{case}/DS_test_NN_ensemble_{nb_ensemble}")   

    t_whole = time.time()-ti_whole
    print(f"Test data for the case {case} have been generated in {int(t_whole//3600)}h:{int((t_whole%3600)//60)}min:{int(((t_whole%3600)%60))}sec.\n")